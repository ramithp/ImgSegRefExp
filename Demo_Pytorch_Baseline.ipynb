{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Forward pass check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.randn(10, 5)\n",
    "\n",
    "featmap_H, featmap_W = 4, 4 \n",
    "\n",
    "feat_lang = data.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, featmap_H, featmap_W)\n",
    "\n",
    "# 10, 4, 4, 5\n",
    "# 10, 5, 4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 4, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_lang.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ImgSegRefExpModel\n",
    "model = ImgSegRefExpModel(mlp_hidden=5, vocab_size=10, emb_size=9, lstm_hidden_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (torch.randn(2, 3, 512, 512), torch.randint(low=0, high=9, size=(2, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([2, 1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "out = model(data)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo for Pytorch implementation of baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "vocab_file = 'vocabulary_referit.txt'\n",
    "# Load vocabulary\n",
    "vocab_dict = load_vocab_dict_from_file(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image and query text\n",
    "# Just need file name. Everything else is auto-magic\n",
    "new_file = '10002.jpg'  #@param {type:\"string\"}\n",
    "query = 'water' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ImgSegRefExpModel:\n\tMissing key(s) in state_dict: \"mlp1.0.weight\", \"mlp2.0.weight\". \n\tUnexpected key(s) in state_dict: \"mlp.0.weight\", \"mlp.0.bias\", \"mlp.1.weight\", \"mlp.1.bias\". \n\tsize mismatch for img_features.feature_extractor.vgg_fc7_full_conv.0.0.weight: copying a param with shape torch.Size([4096, 25088]) from checkpoint, the shape in current model is torch.Size([4096, 512, 7, 7]).\n\tsize mismatch for img_features.feature_extractor.vgg_fc7_full_conv.1.0.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 4096, 1, 1]).\n\tsize mismatch for img_features.feature_extractor.vgg_fc8_full_conv.weight: copying a param with shape torch.Size([1000, 4096]) from checkpoint, the shape in current model is torch.Size([1000, 4096, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4b3736174690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImgSegRefExpModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8803\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_hidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 769\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ImgSegRefExpModel:\n\tMissing key(s) in state_dict: \"mlp1.0.weight\", \"mlp2.0.weight\". \n\tUnexpected key(s) in state_dict: \"mlp.0.weight\", \"mlp.0.bias\", \"mlp.1.weight\", \"mlp.1.bias\". \n\tsize mismatch for img_features.feature_extractor.vgg_fc7_full_conv.0.0.weight: copying a param with shape torch.Size([4096, 25088]) from checkpoint, the shape in current model is torch.Size([4096, 512, 7, 7]).\n\tsize mismatch for img_features.feature_extractor.vgg_fc7_full_conv.1.0.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 4096, 1, 1]).\n\tsize mismatch for img_features.feature_extractor.vgg_fc8_full_conv.weight: copying a param with shape torch.Size([1000, 4096]) from checkpoint, the shape in current model is torch.Size([1000, 4096, 1, 1])."
     ]
    }
   ],
   "source": [
    "from model import ImgSegRefExpModel\n",
    "pretrained_model_file = \"text_objseg_pretrained_torch_converted_with_lstm\"\n",
    "\n",
    "model = ImgSegRefExpModel(mlp_hidden=500, vocab_size=8803, emb_size=1000, lstm_hidden_size=1000)\n",
    "model.load_state_dict(torch.load(pretrained_model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['text_features.embedding.weight', 'text_features.lstm.weight_ih_l0', 'text_features.lstm.weight_hh_l0', 'text_features.lstm.bias_ih_l0', 'text_features.lstm.bias_hh_l0', 'img_features.feature_extractor.0.weight', 'img_features.feature_extractor.0.bias', 'img_features.feature_extractor.2.weight', 'img_features.feature_extractor.2.bias', 'img_features.feature_extractor.5.weight', 'img_features.feature_extractor.5.bias', 'img_features.feature_extractor.7.weight', 'img_features.feature_extractor.7.bias', 'img_features.feature_extractor.10.weight', 'img_features.feature_extractor.10.bias', 'img_features.feature_extractor.12.weight', 'img_features.feature_extractor.12.bias', 'img_features.feature_extractor.14.weight', 'img_features.feature_extractor.14.bias', 'img_features.feature_extractor.17.weight', 'img_features.feature_extractor.17.bias', 'img_features.feature_extractor.19.weight', 'img_features.feature_extractor.19.bias', 'img_features.feature_extractor.21.weight', 'img_features.feature_extractor.21.bias', 'img_features.feature_extractor.24.weight', 'img_features.feature_extractor.24.bias', 'img_features.feature_extractor.26.weight', 'img_features.feature_extractor.26.bias', 'img_features.feature_extractor.28.weight', 'img_features.feature_extractor.28.bias', 'img_features.feature_extractor.vgg_fc7_full_conv.0.0.weight', 'img_features.feature_extractor.vgg_fc7_full_conv.0.0.bias', 'img_features.feature_extractor.vgg_fc7_full_conv.1.0.weight', 'img_features.feature_extractor.vgg_fc7_full_conv.1.0.bias', 'img_features.feature_extractor.vgg_fc8_full_conv.weight', 'img_features.feature_extractor.vgg_fc8_full_conv.bias', 'mlp1.0.weight', 'mlp2.0.weight', 'deconv.dconv.weight'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['img_features.feature_extractor.0.weight', 'img_features.feature_extractor.0.bias', 'img_features.feature_extractor.2.weight', 'img_features.feature_extractor.2.bias', 'img_features.feature_extractor.5.weight', 'img_features.feature_extractor.5.bias', 'img_features.feature_extractor.7.weight', 'img_features.feature_extractor.7.bias', 'img_features.feature_extractor.10.weight', 'img_features.feature_extractor.10.bias', 'img_features.feature_extractor.12.weight', 'img_features.feature_extractor.12.bias', 'img_features.feature_extractor.14.weight', 'img_features.feature_extractor.14.bias', 'img_features.feature_extractor.17.weight', 'img_features.feature_extractor.17.bias', 'img_features.feature_extractor.19.weight', 'img_features.feature_extractor.19.bias', 'img_features.feature_extractor.21.weight', 'img_features.feature_extractor.21.bias', 'img_features.feature_extractor.24.weight', 'img_features.feature_extractor.24.bias', 'img_features.feature_extractor.26.weight', 'img_features.feature_extractor.26.bias', 'img_features.feature_extractor.28.weight', 'img_features.feature_extractor.28.bias', 'img_features.feature_extractor.vgg_fc7_full_conv.0.0.weight', 'img_features.feature_extractor.vgg_fc7_full_conv.0.0.bias', 'img_features.feature_extractor.vgg_fc7_full_conv.1.0.weight', 'img_features.feature_extractor.vgg_fc7_full_conv.1.0.bias', 'img_features.feature_extractor.vgg_fc8_full_conv.weight', 'img_features.feature_extractor.vgg_fc8_full_conv.bias', 'text_features.embedding.weight', 'text_features.lstm.weight_ih_l0', 'text_features.lstm.weight_hh_l0', 'text_features.lstm.bias_ih_l0', 'text_features.lstm.bias_hh_l0', 'mlp.0.weight', 'mlp.0.bias', 'mlp.1.weight', 'mlp.1.bias', 'deconv.dconv.weight'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"text_objseg_pretrained_torch_converted_with_lstm\"\n",
    "torch.load(file).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-676c95c22d49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Run on the input image and query text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtext_seq_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mimcrop_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_H\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "im_file = new_file\n",
    "\n",
    "# Run on the input image and query text\n",
    "text_seq_val = np.zeros((T, N), dtype=np.float32)\n",
    "imcrop_val = np.zeros((N, input_H, input_W, 3), dtype=np.float32)\n",
    "\n",
    "# Preprocess image and text\n",
    "im = skimage.io.imread(im_file)\n",
    "processed_im = skimage.img_as_ubyte(im_processing.resize_and_pad(im, input_H, input_W))\n",
    "imcrop_val[0, :] = processed_im.astype(np.float32) - segmodel.vgg_net.channel_mean\n",
    "text_seq_val[:, 0] = utils.preprocess_sentence(query, vocab_dict, T)\n",
    "\n",
    "# Forward pass to get response map\n",
    "scores_val = np.squeeze(sess.run(scores, feed_dict={text_seq_batch: text_seq_val,\n",
    "                                                    imcrop_batch: imcrop_val}))\n",
    "\n",
    "output = model((imcrop_val, text_seq_val))\n",
    "\n",
    "# Final prediction\n",
    "prediction = im_processing.resize_and_crop(scores_val>0, *im.shape[:2]).astype(np.bool)\n",
    "print(prediction)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(im)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(prediction)\n",
    "\n",
    "print(\"query text = '%s'\" % query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
