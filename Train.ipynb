{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os; \n",
    "import numpy as np\n",
    "from data import ImageSegmentationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model import ImgSegRefExpModel\n",
    "\n",
    "\n",
    "def load_vocab_dict_from_file(dict_file):\n",
    "    with open(dict_file) as f:\n",
    "        words = [w.strip() for w in f.readlines()]\n",
    "    vocab_dict = {words[n]:n for n in range(len(words))}\n",
    "    return vocab_dict\n",
    "\n",
    "#Util functions\n",
    "\n",
    "# # all boxes are [num, height, width] binary array\n",
    "def compute_mask_IU(masks, target):\n",
    "    #print (np.sum(np.logical_and(masks, target)))\n",
    "    assert(target.shape[-2:] == masks.shape[-2:])\n",
    "    I = torch.sum(np.logical_and(masks, target))\n",
    "    U = torch.sum(np.logical_or(masks, target))\n",
    "    return I, U\n",
    "\n",
    "def resize_and_pad(im, input_h, input_w):\n",
    "    # Resize and pad im to input_h x input_w size\n",
    "    im_h, im_w = im.shape[:2]\n",
    "    scale = min(input_h / im_h, input_w / im_w)\n",
    "    resized_h = int(np.round(im_h * scale))\n",
    "    resized_w = int(np.round(im_w * scale))\n",
    "    pad_h = int(np.floor(input_h - resized_h) / 2)\n",
    "    pad_w = int(np.floor(input_w - resized_w) / 2)\n",
    "\n",
    "    resized_im = skimage.transform.resize(im, [resized_h, resized_w])\n",
    "    if im.ndim > 2:\n",
    "        new_im = np.zeros((input_h, input_w, im.shape[2]), dtype=resized_im.dtype)\n",
    "    else:\n",
    "        new_im = np.zeros((input_h, input_w), dtype=resized_im.dtype)\n",
    "    new_im[pad_h:pad_h+resized_h, pad_w:pad_w+resized_w, ...] = resized_im\n",
    "\n",
    "    return new_im\n",
    "\n",
    "def resize_and_crop(im, input_h, input_w):\n",
    "    # Resize and crop im to input_h x input_w size\n",
    "    im_h, im_w = im.shape[:2]\n",
    "    scale = max(input_h / im_h, input_w / im_w)\n",
    "    resized_h = int(np.round(im_h * scale))\n",
    "    resized_w = int(np.round(im_w * scale))\n",
    "    crop_h = int(np.floor(resized_h - input_h) / 2)\n",
    "    crop_w = int(np.floor(resized_w - input_w) / 2)\n",
    "\n",
    "    resized_im = skimage.transform.resize(im, [resized_h, resized_w])\n",
    "    if im.ndim > 2:\n",
    "        new_im = np.zeros((input_h, input_w, im.shape[2]), dtype=resized_im.dtype)\n",
    "    else:\n",
    "        new_im = np.zeros((input_h, input_w), dtype=resized_im.dtype)\n",
    "    new_im[...] = resized_im[crop_h:crop_h+input_h, crop_w:crop_w+input_w, ...]\n",
    "\n",
    "    return new_im\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# Parameters\n",
    "################################################################################\n",
    "\n",
    "root = '/home/nishaddawkhar/text_objseg/exp-referit/'\n",
    "\n",
    "image_dir = root + 'referit-dataset/images/'\n",
    "mask_dir = root + 'referit-dataset/mask/'\n",
    "query_file = root + 'data/referit_query_train.json'\n",
    "bbox_file = root + 'data/referit_bbox.json'\n",
    "imcrop_file = root + 'data/referit_imcrop.json'\n",
    "imsize_file = root + 'data/referit_imsize.json'\n",
    "vocab_file = root + 'data/vocabulary_referit.txt'\n",
    "\n",
    "query_file_val = root + 'data/referit_query_val.json'\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(query_file, image_dir, mask_dir)\n",
    "val_dataset = ImageSegmentationDataset(query_file_val, image_dir, mask_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saving image sizes\n",
    "# query_dict = json.load(open(query_file))\n",
    "\n",
    "# image_names = set()\n",
    "# for key, value in query_dict.items():\n",
    "#     image_names.add(key.split('_')[0])\n",
    "\n",
    "# import skimage.io\n",
    "# image_sizes = {}\n",
    "# for name in image_names:\n",
    "#     im = skimage.io.imread(image_dir + name + '.jpg')\n",
    "#     image_sizes[name] = im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImgSegRefExpModel(\n",
       "  (text_features): LanguageModule(\n",
       "    (embedding): Embedding(8803, 1000)\n",
       "    (lstm): LSTM(1000, 1000, batch_first=True)\n",
       "  )\n",
       "  (img_features): ImageModule(\n",
       "    (feature_extractor): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (vgg_fc7_full_conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "          (1): ReLU(inplace)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (vgg_fc8_full_conv): Conv2d(4096, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): Conv2d(2008, 500, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "  )\n",
       "  (mlp2): Sequential(\n",
       "    (0): Conv2d(500, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (deconv): DeconvLayer(\n",
       "    (dconv): ConvTranspose2d(1, 1, kernel_size=(64, 64), stride=(32, 32), padding=(16, 16), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# trained model\n",
    "pretrained_model_file = \"/home/nishaddawkhar/text_objseg_pretrained_torch_converted_with_lstm.dms\"\n",
    "vocab_file = './vocabulary_referit.txt'\n",
    "\n",
    "# Load vocabulary\n",
    "vocab_dict = load_vocab_dict_from_file(vocab_file)\n",
    "\n",
    "# Load model and weights\n",
    "model = ImgSegRefExpModel(mlp_hidden=500, vocab_size=8803, emb_size=1000, lstm_hidden_size=1000)\n",
    "pre_trained = torch.load(pretrained_model_file)\n",
    "model.load_state_dict(pre_trained)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Number of batches: 5414\tBatch Size: 10\tDataset size: 54134\n",
      "Validating\n",
      "Number of batches: 585\tBatch Size: 10\tDataset size: 5842\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "Batch #0: Loss = 0.0009770632022991776\tAvg Loss: 9.770632022991785e-06\tTime: 0.9425015449523926s\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([10, 1000, 16, 16])\n",
      "16 16\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# Model Params\n",
    "T = 20\n",
    "N = 10\n",
    "input_H = 512; featmap_H = (input_H // 32)\n",
    "input_W = 512; featmap_W = (input_W // 32)\n",
    "num_vocab = 8803\n",
    "embed_dim = 1000\n",
    "lstm_dim = 1000\n",
    "mlp_hidden_dims = 500\n",
    "\n",
    "#Training Params\n",
    "pos_loss_mult = 1.\n",
    "neg_loss_mult = 1.\n",
    "\n",
    "start_lr = 0.01\n",
    "lr_decay_step = 10000\n",
    "lr_decay_rate = 0.1\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "max_iter = 30000\n",
    "\n",
    "fix_convnet = False\n",
    "vgg_dropout = False\n",
    "mlp_dropout = False\n",
    "vgg_lr_mult = 1.\n",
    "\n",
    "cls_loss_avg = 0\n",
    "avg_accuracy_all, avg_accuracy_pos, avg_accuracy_neg = 0, 0, 0\n",
    "decay = 0.99\n",
    "\n",
    "# Combine weight decay regularisation with optimiser\n",
    "optimiser = torch.optim.SGD(model.parameters(),lr=start_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "torch.optim.lr_scheduler.StepLR(optimiser, step_size=lr_decay_step, gamma=lr_decay_rate)\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(int(pos_loss_mult),int(neg_loss_mult)).to(device))\n",
    "\n",
    "for n_iter in range(max_iter):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    print(\"Training\\nNumber of batches: {}\\tBatch Size: {}\\tDataset size: {}\".format(len(train_loader),train_loader.batch_size,len(train_loader.dataset)))\n",
    "    cls_loss_avg = 0.0\n",
    "    end_time = 0\n",
    "#     for batchId,(image, text, gt_mask, original_image_name) in enumerate(train_loader):\n",
    "#         optimiser.zero_grad()\n",
    "#         batch_start = time.time()\n",
    "#         text = text.long()\n",
    "#         output_mask = model((image.to(device), text.to(device)))\n",
    "#         output_mask = output_mask.squeeze(1)\n",
    "#         cls_loss_val = loss(output_mask,gt_mask.float().to(device))\n",
    "#         cls_loss_val.backward()\n",
    "#         cls_loss_avg = decay*cls_loss_avg + (1-decay)*cls_loss_val.item()\n",
    "#         optimiser.step()\n",
    "# #         import pdb; pdb.set_trace();\n",
    "#         print(\"Batch Time with data loading = {}s, Batch #{}: Loss = {}\\tAvg Loss: {}\\tTime: {}s\".format(time.time()-end_time,batchId,cls_loss_val.item(),cls_loss_avg,time.time()-batch_start))\n",
    "#         end_time = time.time()\n",
    "#     print('\\titer = {},  Batch Loss (avg) = {}, lr = {}, time = {}s'.format(n_iter, cls_loss_avg, get_lr(optimiser),time.time()-start))\n",
    "    \n",
    "    print(\"Validating\\nNumber of batches: {}\\tBatch Size: {}\\tDataset size: {}\".format(len(val_loader),val_loader.batch_size,len(val_loader.dataset)))\n",
    "    cls_loss_avg = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_start = time.time()\n",
    "        for batchId,(image, text, gt_mask, original_image_name) in enumerate(val_loader):\n",
    "            optimiser.zero_grad()\n",
    "            text = text.long()\n",
    "            output_mask = model((image.to(device), text.to(device)))\n",
    "            output_mask = output_mask.squeeze(1)\n",
    "            cls_loss_val = loss(output_mask,gt_mask.float().to(device))\n",
    "            cls_loss_avg = decay*cls_loss_avg + (1-decay)*cls_loss_val.item()\n",
    "            if batchId % 100 == 0:\n",
    "                print(\"Batch #{}: Loss = {}\\tAvg Loss: {}\\tTime: {}s\".format(batchId,cls_loss_val.item(),cls_loss_avg,time.time()-batch_start))\n",
    "        print('\\titer = {},  Batch Loss (avg) = {}, lr = {}, time = {}s'.format(n_iter, cls_loss_avg, get_lr(optimiser),time.time()))   \n",
    "print('Optimization done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
