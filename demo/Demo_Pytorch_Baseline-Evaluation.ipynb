{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo for Pytorch implementation of baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add main code-base to path\n",
    "sys.path.append('../')\n",
    "\n",
    "from util_rename import *\n",
    "from model import ImgSegRefExpModel\n",
    "import torch\n",
    "import sys\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import timeit\n",
    "from utl import eval_tools, im_processing\n",
    "#import data\n",
    "from data import ImageSegmentationDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained model\n",
    "pretrained_model_file = \"/Users/shubhammehrotra/Downloads/text_objseg_pretrained_torch_converted_with_lstm\"\n",
    "vocab_file = './vocabulary_referit.txt'\n",
    "\n",
    "root = '/Users/shubhammehrotra/text_objseg/exp-referit/'\n",
    "\n",
    "image_dir = root + 'referit-dataset/images/'\n",
    "mask_dir = root + 'referit-dataset/mask/'\n",
    "query_file = root + 'data/referit_query_test.json'\n",
    "bbox_file = root + 'data/referit_bbox.json'\n",
    "imcrop_file = root + 'data/referit_imcrop.json'\n",
    "imsize_file = root + 'data/referit_imsize.json'\n",
    "vocab_file = root + 'data/vocabulary_referit.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "vocab_dict = load_vocab_dict_from_file(vocab_file)\n",
    "\n",
    "# Load model and weights\n",
    "model = ImgSegRefExpModel(mlp_hidden=500, vocab_size=8803, emb_size=1000, lstm_hidden_size=1000)\n",
    "pre_trained = torch.load(pretrained_model_file)\n",
    "model.load_state_dict(pre_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model.state_dict().keys()).difference(set(pre_trained.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageSegmentationDataset(query_file, image_dir, mask_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "sum_I = 0\n",
    "sum_U = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name /Users/shubhammehrotra/text_objseg/exp-referit/referit-dataset/images/30456\n",
      "Image Text sky\n",
      "Image Number: 0\n",
      "torch.Size([1, 1000, 16, 16])\n",
      "16 16\n",
      "torch.Size([1, 1000, 16, 16])\n",
      "16 16\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * ()\n * (torch.dtype dtype)\n * (tuple of ints dim, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: out, axis\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-88933f36d8c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#print (prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_processing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_and_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask_IU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ImgSegRefExp/utl/eval_tools.py\u001b[0m in \u001b[0;36mcompute_mask_IU\u001b[0;34m(masks, target)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask_IU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2076\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * ()\n * (torch.dtype dtype)\n * (tuple of ints dim, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: out, axis\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (image, text, mask, original_image) in enumerate(test_loader):\n",
    "    print ('Image Number:', batch_idx)\n",
    "    text = text.long()\n",
    "    output_mask = model((image, text))\n",
    "    output_mask = model((image, text))\n",
    "    \n",
    "    labels = mask > 0\n",
    "    #labels = mask.cpu().detach().numpy() > 0\n",
    "    #print(\"Mask shape\", output_mask.shape)\n",
    "    output_mask = output_mask.squeeze(0)\n",
    "    output_mask = output_mask.squeeze(0)\n",
    "    #print(\"Mask shape squeezed\", output_mask.shape)\n",
    "    \n",
    "    image_name = \"\"\n",
    "    for digit in original_image[0]:\n",
    "        #print (digit.item())\n",
    "        image_name += str(digit.item())\n",
    "\n",
    "    #print (int (q))\n",
    "    im = skimage.io.imread('/Users/shubhammehrotra/text_objseg/exp-referit/referit-dataset/images/' + image_name + '.jpg')\n",
    "    \n",
    "    #prediction = resize_and_crop(output_mask > 0, * im.shape[:2]).astype(np.bool)\n",
    "    # Final prediction\n",
    "    prediction = resize_and_crop(output_mask.cpu().detach().numpy() > 0, * im.shape[:2]).astype(np.bool)\n",
    "    #print (prediction)\n",
    "    prediction = im_processing.resize_and_crop(prediction, im.shape[0], im.shape[1])\n",
    "    I, U = eval_tools.compute_mask_IU(prediction, labels)\n",
    "    I = float(I)\n",
    "    U = float(U)\n",
    "    sum_I += I\n",
    "    sum_U += U\n",
    "    print ('Image IOU', I / U)\n",
    "    print ('Updated IOU', sum_I / sum_U)\n",
    "    print ('\\n')\n",
    "    break\n",
    "\n",
    "print ('Final IOU', float(sum_I)/sum_U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying output\n",
    "   \n",
    "im = skimage.io.imread('/Users/shubhammehrotra/text_objseg/exp-referit/referit-dataset/images/' + q + '.jpg')\n",
    "\n",
    "    # Final prediction\n",
    "prediction = resize_and_crop(output_mask.cpu().detach().numpy() > 0, * im.shape[:2]).astype(np.bool)\n",
    "#print (prediction)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(im)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(prediction)\n",
    "#print (labels.shape, prediction.shape)\n",
    "#print ()\n",
    "prediction = im_processing.resize_and_crop(prediction, im.shape[0], im.shape[1])\n",
    "I, U = eval_tools.compute_mask_IU(prediction, labels)\n",
    "\n",
    "print ('IOU',  I/U)\n",
    "\n",
    "\n",
    "#         # Evaluate the segmentation performance of using bounding box segmentation\n",
    "#         pred_raw = (scores_val >= score_thresh).astype(np.float32)\n",
    "#         predicts = im_processing.resize_and_crop(pred_raw, im.shape[0], im.shape[1])\n",
    "#         I, U = eval_tools.compute_mask_IU(predicts, labels)\n",
    "#         cum_I += I\n",
    "#         cum_U += U\n",
    "#         this_IoU = I/U\n",
    "\n",
    "#print(\"query text = '%s'\" % query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_name = \"12346\"\n",
    "char = []\n",
    "    \n",
    "char = [int(i) for i in original_image_name]\n",
    "\n",
    "char = torch.from_numpy(np.array(char))\n",
    "\n",
    "q = \"\"\n",
    "for digit in s:\n",
    "    print (digit)\n",
    "    q += str(digit)\n",
    "\n",
    "print (int (q))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image and query text\n",
    "im_file = './38100.jpg'\n",
    "query = 'sky above the bridge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# Model Param\n",
    "T = 20\n",
    "N = 1\n",
    "input_H = 512; featmap_H = (input_H // 32)\n",
    "input_W = 512; featmap_W = (input_W // 32)\n",
    "num_vocab = 8803\n",
    "embed_dim = 1000\n",
    "lstm_dim = 1000\n",
    "mlp_hidden_dims = 500\n",
    "\n",
    "channel_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "\n",
    "# Run on the input image and query text\n",
    "text_seq_val = np.zeros((T, N), dtype=np.float32)\n",
    "imcrop_val = np.zeros((N, input_H, input_W, 3), dtype=np.float32)\n",
    "\n",
    "# Preprocess image and text\n",
    "im = skimage.io.imread(im_file)\n",
    "\n",
    "# Makes it uint8\n",
    "processed_im = skimage.img_as_ubyte(resize_and_pad(im, input_H, input_W))\n",
    "print(processed_im.dtype)\n",
    "\n",
    "imcrop_val[0, :] = processed_im.astype(np.float32) - channel_mean\n",
    "print(imcrop_val.dtype)\n",
    "\n",
    "# preprocess sentence pads 0's before the phrase. flipping it ruins the output\n",
    "text_seq_val[:, 0] = preprocess_sentence(query, vocab_dict, T)\n",
    "print(text_seq_val.dtype)\n",
    "print (text_seq_val)\n",
    "\n",
    "imcrop_val = torch.Tensor(imcrop_val).permute(0, 3, 1, 2) \n",
    "text_seq_val = torch.LongTensor(text_seq_val).t() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imcrop_val.shape)\n",
    "print(text_seq_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get response map\n",
    "output_mask = model((imcrop_val, text_seq_val))\n",
    "\n",
    "print(\"Mask shape\", output_mask.shape)\n",
    "output_mask = output_mask.squeeze(0)\n",
    "output_mask = output_mask.squeeze(0)\n",
    "print(\"Mask shape squeezed\", output_mask.shape)\n",
    "\n",
    "# Final prediction\n",
    "prediction = resize_and_crop(output_mask.cpu().detach().numpy() > 0, * im.shape[:2]).astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(im)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(prediction)\n",
    "\n",
    "print(\"query text = '%s'\" % query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
